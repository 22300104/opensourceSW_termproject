import cv2
import mediapipe as mp
import numpy as np
import os
import sys
from PIL import Image, ImageDraw, ImageFont
from datetime import datetime

# --- [1. ì„¤ì • ë° ì´ˆê¸°í™”] ---
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    max_num_faces=1,
    refine_landmarks=True,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# [ê°€ìƒ ë°°ê²½] ì„¸ê·¸ë©˜í…Œì´ì…˜ ì´ˆê¸°í™”
mp_selfie_segmentation = mp.solutions.selfie_segmentation
selfie_segmentation = mp_selfie_segmentation.SelfieSegmentation(model_selection=1)

# [ì œìŠ¤ì²˜ ì»¨íŠ¸ë¡¤] ì† ì¸ì‹ ì´ˆê¸°í™”
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    max_num_hands=1,
    min_detection_confidence=0.7, # ì œìŠ¤ì²˜ ì˜¤ì‘ë™ ë°©ì§€ë¥¼ ìœ„í•´ ì •í™•ë„ ë†’ì„
    min_tracking_confidence=0.5
)

# --- [2. í•„í„° ìƒì„± í•¨ìˆ˜ë“¤] ---
def create_glasses_filter(width, height):
    """ì•ˆê²½ í•„í„° ìƒì„±"""
    img = np.zeros((height, width, 4), dtype=np.uint8)
    lens_radius = min(width, height) // 4
    cv2.circle(img, (width//4, height//2), lens_radius, (50, 50, 50, 200), -1)
    cv2.circle(img, (3*width//4, height//2), lens_radius, (50, 50, 50, 200), -1)
    cv2.circle(img, (width//4, height//2), lens_radius, (0, 0, 0, 255), 3)
    cv2.circle(img, (3*width//4, height//2), lens_radius, (0, 0, 0, 255), 3)
    cv2.line(img, (width//4 - lens_radius, height//2), (0, height//2), (0, 0, 0, 255), 3)
    cv2.line(img, (3*width//4 + lens_radius, height//2), (width, height//2), (0, 0, 0, 255), 3)
    cv2.line(img, (width//4, height//2 - lens_radius//2), (3*width//4, height//2 - lens_radius//2), (0, 0, 0, 255), 3)
    return img

def create_hat_filter(width, height):
    """ëª¨ì í•„í„° ìƒì„±"""
    img = np.zeros((height, width, 4), dtype=np.uint8)
    cv2.ellipse(img, (width//2, height//3), (width//2, height//3), 0, 0, 180, (139, 69, 19, 255), -1)
    cv2.ellipse(img, (width//2, height//3), (width//2, height//3), 0, 0, 180, (0, 0, 0, 255), 3)
    cv2.rectangle(img, (width//2 - 20, height//3 - 5), (width//2 + 20, height//3 + 5), (255, 0, 0, 255), -1)
    return img

def create_mustache_filter(width, height):
    """ìˆ˜ì—¼ í•„í„° ìƒì„±"""
    img = np.zeros((height, width, 4), dtype=np.uint8)
    cv2.ellipse(img, (width//2, height//2), (width//3, height//4), 0, 0, 360, (50, 50, 50, 220), -1)
    cv2.ellipse(img, (width//2, height//2), (width//3, height//4), 0, 0, 360, (0, 0, 0, 255), 2)
    cv2.ellipse(img, (width//4, height//2), (width//8, height//6), 0, 0, 360, (50, 50, 50, 220), -1)
    cv2.ellipse(img, (3*width//4, height//2), (width//8, height//6), 0, 0, 360, (50, 50, 50, 220), -1)
    return img

def create_crown_filter(width, height):
    """ì™•ê´€ í•„í„° ìƒì„±"""
    img = np.zeros((height, width, 4), dtype=np.uint8)
    points = np.array([
        [width//2, 0], [width//2 - width//4, height//2], [width//4, height//2],
        [width//2, height//3], [3*width//4, height//2], [width//2 + width//4, height//2]
    ], np.int32)
    cv2.fillPoly(img, [points], (255, 215, 0, 255))
    cv2.polylines(img, [points], True, (0, 0, 0, 255), 2)
    cv2.circle(img, (width//2, height//6), 5, (255, 0, 0, 255), -1)
    cv2.circle(img, (width//4, height//3), 4, (0, 255, 0, 255), -1)
    cv2.circle(img, (3*width//4, height//3), 4, (0, 0, 255, 255), -1)
    return img

def distort_region(image, cx, cy, radius, strength=0.5):
    """ì˜ì—­ ì™œê³¡ (ì™•ëˆˆì´ íš¨ê³¼)"""
    try:
        h, w = image.shape[:2]
        x1, x2 = max(0, cx - radius), min(w, cx + radius)
        y1, y2 = max(0, cy - radius), min(h, cy + radius)
        if x2 <= x1 or y2 <= y1: return image
        
        roi = image[y1:y2, x1:x2]
        rh, rw = roi.shape[:2]
        grid_y, grid_x = np.indices((rh, rw), dtype=np.float32)
        
        rel_cx, rel_cy = cx - x1, cy - y1
        r = np.sqrt((grid_x - rel_cx)**2 + (grid_y - rel_cy)**2)
        mask = r < radius
        
        map_x, map_y = grid_x.copy(), grid_y.copy()
        factor = 1.0 - strength * (1.0 - r[mask] / radius)
        map_x[mask] = (grid_x[mask] - rel_cx) * factor + rel_cx
        map_y[mask] = (grid_y[mask] - rel_cy) * factor + rel_cy
        
        distorted = cv2.remap(roi, map_x, map_y, cv2.INTER_LINEAR)
        np.copyto(roi, distorted, where=np.stack((mask,)*3, axis=-1))
        image[y1:y2, x1:x2] = roi
    except: pass
    return image

# --- [3. í•µì‹¬ í•¨ìˆ˜: íˆ¬ëª… ì´ë¯¸ì§€ í•©ì„±] ---
def overlay_transparent(background, overlay, x, y):
    try:
        bg_h, bg_w, _ = background.shape
        h, w, _ = overlay.shape

        if x < 0: 
            overlay = overlay[:, -x:]
            w = overlay.shape[1]
            x = 0
        if y < 0:
            overlay = overlay[-y:, :]
            h = overlay.shape[0]
            y = 0
        if x + w > bg_w:
            overlay = overlay[:, :bg_w - x]
            w = overlay.shape[1]
        if y + h > bg_h:
            overlay = overlay[:bg_h - y, :]
            h = overlay.shape[0]

        alpha = overlay[:, :, 3] / 255.0
        colors = overlay[:, :, :3]
        roi = background[y:y+h, x:x+w]
        
        for c in range(0, 3):
            roi[:, :, c] = roi[:, :, c] * (1.0 - alpha) + colors[:, :, c] * alpha
            
        background[y:y+h, x:x+w] = roi
        return background
    except Exception as e:
        return background

# --- [4. ì´ë¯¸ì§€ íŒŒì¼ ë¡œë“œ] ---
def load_filter_image(filename, default_width=300, default_height=100, create_func=None):
    current_dir = os.getcwd()
    possible_paths = [filename, os.path.join('Sejung', filename), os.path.join('..', filename)]
    img = None
    for path in possible_paths:
        if os.path.exists(path):
            img = cv2.imread(path, cv2.IMREAD_UNCHANGED)
            if img is not None:
                print(f"âœ… ì´ë¯¸ì§€ ë¡œë“œ ì„±ê³µ: {path}")
                break
    if img is None and create_func:
        print(f"âš ï¸ '{filename}' ëŒ€ì²´ ì½”ë“œ ì‚¬ìš©")
        img = create_func(default_width, default_height)
    elif img is not None and img.shape[2] < 4:
        img = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)
    return img

print("\n=== ë¦¬ì†ŒìŠ¤ ë¡œë“œ ì¤‘ ===")
glasses_img = load_filter_image('glasses.png', 300, 100, create_glasses_filter)
hat_img = load_filter_image('hat.png', 300, 180, create_hat_filter)
mustache_img = load_filter_image('mustache.png', 300, 150, create_mustache_filter)
crown_img = load_filter_image('crown.png', 300, 240, create_crown_filter)
print("=" * 30 + "\n")

# --- [5. UI ë° í…ìŠ¤íŠ¸ ê´€ë ¨] ---
def put_korean_text(img, text, position, font_size=30, color=(0, 255, 0), align='left'):
    try:
        img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(img_pil)
        
        font = None
        font_paths = ["C:/Windows/Fonts/malgun.ttf", "malgun.ttf", "gulim.ttc"]
        for font_path in font_paths:
            try:
                if os.path.exists(font_path):
                    font = ImageFont.truetype(font_path, font_size)
                    break
            except: continue
        if font is None: font = ImageFont.load_default()

        # ì •ë ¬ ì²˜ë¦¬
        x, y = position
        if align == 'center':
            bbox = draw.textbbox((0, 0), text, font=font)
            text_width = bbox[2] - bbox[0]
            x = x - text_width // 2
        
        draw.text((x, y), text, font=font, fill=color)
        img = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
    except: pass
    return img

# --- [6. í•„í„° ì„¤ì •] ---
FILTER_SETTINGS = {
    'glasses': {'size_ratio': 2.3, 'height_ratio': 0.4, 'offset_x': 0, 'offset_y': 0},
    'hat': {'size_ratio': 2.5, 'height_ratio': 0.6, 'offset_x': 0, 'offset_y': 60},
    'mustache': {'size_ratio': 1.8, 'height_ratio': 0.5, 'offset_x': 0, 'offset_y': -25},
    'crown': {'size_ratio': 2.2, 'height_ratio': 0.8, 'offset_x': 0, 'offset_y': 15}
}

SIZE_SCALE = 1.0
ALPHA_SCALE = 1.0
SIZE_STEP = 0.1
ALPHA_STEP = 0.1
SIZE_MIN, SIZE_MAX = 0.5, 3.0
ALPHA_MIN, ALPHA_MAX = 0.1, 2.0

SCREENSHOT_DIR = "screenshots"
SCREENSHOT_FMT = "jpg"
SCREENSHOT_QUALITY = 95
RECORD_DIR = "videos"
RECORD_CODEC = "mp4v"
RECORD_FPS_FALLBACK = 30

# --- [7. ìƒíƒœ ê´€ë¦¬] ---
FILTER_ITEMS = ['glasses', 'hat', 'mustache', 'crown', 'big_eyes']
FILTER_LABELS = ['ì•ˆê²½', 'ëª¨ì', 'ìˆ˜ì—¼', 'ì™•ê´€', 'ì™•ëˆˆì´']
current_cursor_index = 0
active_filters = ['glasses']
background_mode = 0
BACKGROUND_MODES = {0: "ì—†ìŒ", 1: "ë¸”ëŸ¬", 2: "í•‘í¬", 3: "ë°¤í•˜ëŠ˜"}

# ì œìŠ¤ì²˜ ì¿¨íƒ€ì„ ê´€ë¦¬
gesture_cooldown = 0
GESTURE_LOCK_TIME = 20 # í”„ë ˆì„

# --- [ì œìŠ¤ì²˜ ì¸ì‹ í•¨ìˆ˜] ---
def detect_gesture(hand_landmarks):
    """
    ì† ëœë“œë§ˆí¬ë¥¼ ë¶„ì„í•˜ì—¬ ì œìŠ¤ì²˜ ë°˜í™˜
    Return: 'V', 'PALM', 'FIST', 'POINT', 'NONE'
    """
    # ì†ê°€ë½ ë(TIP)ê³¼ ë§ˆë””(PIP) ì¸ë±ìŠ¤
    # ê²€ì§€(8), ì¤‘ì§€(12), ì•½ì§€(16), ì†Œì§€(20)
    tips = [8, 12, 16, 20]
    pips = [6, 10, 14, 18]
    
    # í´ì§ ì—¬ë¶€ í™•ì¸ (í™”ë©´ ì¢Œí‘œê³„: ìœ„ê°€ y ì‘ìŒ)
    is_open = []
    for tip, pip in zip(tips, pips):
        is_open.append(hand_landmarks.landmark[tip].y < hand_landmarks.landmark[pip].y)
    
    # ì œìŠ¤ì²˜ íŒë³„
    # 1. ë³´(PALM): 4ê°œ ëª¨ë‘ í´ì§ -> ë‹¤ìŒ ë©”ë‰´ ì´ë™
    if all(is_open):
        return "PALM"
    # 2. ì£¼ë¨¹(FIST): 4ê°œ ëª¨ë‘ ì ‘í˜ -> ì„ íƒ/í•´ì œ
    if not any(is_open):
        return "FIST"
    # 3. ë¸Œì´(V): ê²€ì§€, ì¤‘ì§€ í´ì§ + ë‚˜ë¨¸ì§€ ì ‘í˜ -> ìŠ¤í¬ë¦°ìƒ·
    if is_open[0] and is_open[1] and not is_open[2] and not is_open[3]:
        return "V"
    # 4. ê²€ì§€(POINT): ê²€ì§€ í´ì§ + ë‚˜ë¨¸ì§€ ì ‘í˜ -> ë°°ê²½ ë³€ê²½
    if is_open[0] and not is_open[1] and not is_open[2] and not is_open[3]:
        return "POINT"
        
    return "NONE"

# --- [ë°°ê²½ ì ìš©] ---
def apply_background(image, mode):
    if mode == 0: return image
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = selfie_segmentation.process(image_rgb)
    mask = results.segmentation_mask
    condition = np.stack((mask,) * 3, axis=-1) > 0.5
    
    h, w, c = image.shape
    if mode == 1:
        bg_image = cv2.GaussianBlur(image, (55, 55), 0)
    elif mode == 2:
        bg_image = np.zeros((h, w, 3), dtype=np.uint8)
        bg_image[:] = (180, 105, 255)
    elif mode == 3:
        bg_image = np.zeros((h, w, 3), dtype=np.uint8)
        for i in range(h):
            color_val = int(100 * (i / h))
            bg_image[i, :] = (50, 20 + color_val, 0)
        np.random.seed(42)
        for _ in range(50):
            cv2.circle(bg_image, (np.random.randint(0, w), np.random.randint(0, h)), 
                      np.random.randint(1, 3), (255, 255, 255), -1)
    
    return np.where(condition, image, bg_image) if 'bg_image' in locals() else image

# --- [í•„í„° ì ìš©] ---
def apply_filter(image, face_landmarks, filter_type, h, w):
    if filter_type == 'none': return image
    
    left_eye = face_landmarks.landmark[33]
    right_eye = face_landmarks.landmark[263]
    lx, ly = int(left_eye.x * w), int(left_eye.y * h)
    rx, ry = int(right_eye.x * w), int(right_eye.y * h)
    
    dx, dy = rx - lx, ry - ly
    angle = np.degrees(np.arctan2(dy, dx))
    eye_dist = np.sqrt(dx**2 + dy**2)
    
    settings = FILTER_SETTINGS.get(filter_type, {})
    size_ratio = settings.get('size_ratio', 2.0) * SIZE_SCALE
    height_ratio = settings.get('height_ratio', 0.5)
    offset_x, offset_y = settings.get('offset_x', 0), settings.get('offset_y', 0)
    
    filter_img = None
    target_width = int(eye_dist * size_ratio)
    
    if filter_type == 'glasses':
        if glasses_img is not None:
            scale = target_width / glasses_img.shape[1]
            filter_img = cv2.resize(glasses_img.copy(), (target_width, int(glasses_img.shape[0] * scale)))
        else: filter_img = create_glasses_filter(target_width, int(target_width * height_ratio))
        center_x = (lx + rx) // 2 - target_width // 2 + offset_x
        center_y = (ly + ry) // 2 - filter_img.shape[0] // 2 + offset_y
        
    elif filter_type == 'hat':
        forehead = face_landmarks.landmark[10]
        fx, fy = int(forehead.x * w), int(forehead.y * h)
        if hat_img is not None:
            scale = target_width / hat_img.shape[1]
            filter_img = cv2.resize(hat_img.copy(), (target_width, int(hat_img.shape[0] * scale)))
        else: filter_img = create_hat_filter(target_width, int(target_width * height_ratio))
        center_x = fx - target_width // 2 + offset_x
        center_y = fy - filter_img.shape[0] + offset_y

    elif filter_type == 'mustache':
        nose = face_landmarks.landmark[4]
        lip = face_landmarks.landmark[13]
        nx, ny = int(nose.x * w), int(nose.y * h)
        ux, uy = int(lip.x * w), int(lip.y * h)
        if mustache_img is not None:
            scale = target_width / mustache_img.shape[1]
            filter_img = cv2.resize(mustache_img.copy(), (target_width, int(mustache_img.shape[0] * scale)))
        else: filter_img = create_mustache_filter(target_width, int(target_width * height_ratio))
        center_x = (nx + ux) // 2 - target_width // 2 + offset_x
        center_y = (ny + uy) // 2 + offset_y

    elif filter_type == 'crown':
        forehead = face_landmarks.landmark[10]
        fx, fy = int(forehead.x * w), int(forehead.y * h)
        if crown_img is not None:
            scale = target_width / crown_img.shape[1]
            filter_img = cv2.resize(crown_img.copy(), (target_width, int(crown_img.shape[0] * scale)))
        else: filter_img = create_crown_filter(target_width, int(target_width * height_ratio))
        center_x = fx - target_width // 2 + offset_x
        center_y = fy - filter_img.shape[0] + offset_y

    elif filter_type == 'big_eyes':
        le = face_landmarks.landmark[468] # ì™¼ìª½ í™ì±„
        re = face_landmarks.landmark[473] # ì˜¤ë¥¸ìª½ í™ì±„
        lx, ly = int(le.x * w), int(le.y * h)
        rx, ry = int(re.x * w), int(re.y * h)
        
        eye_dist = np.sqrt((lx-rx)**2 + (ly-ry)**2)
        radius = int(eye_dist * 0.45 * SIZE_SCALE)
        strength = 0.6 * ALPHA_SCALE
        
        image = distort_region(image, lx, ly, radius, strength)
        image = distort_region(image, rx, ry, radius, strength)
        return image

    if filter_img is not None:
        M = cv2.getRotationMatrix2D((filter_img.shape[1]//2, filter_img.shape[0]//2), -angle, 1)
        rotated = cv2.warpAffine(filter_img, M, (filter_img.shape[1], filter_img.shape[0]))
        if rotated.shape[2] == 4 and ALPHA_SCALE != 1.0:
            rotated[:, :, 3] = np.clip(rotated[:, :, 3] * ALPHA_SCALE, 0, 255)
        image = overlay_transparent(image, rotated, center_x, center_y)
        
    return image

def apply_filters(image, face_landmarks, filters, h, w):
    for f in filters: image = apply_filter(image, face_landmarks, f, h, w)
    return image

def save_screenshot(image, filter_name):
    try:
        if not os.path.exists(SCREENSHOT_DIR): os.makedirs(SCREENSHOT_DIR)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = os.path.join(SCREENSHOT_DIR, f"screenshot_{filter_name}_{timestamp}.{SCREENSHOT_FMT}")
        params = [cv2.IMWRITE_JPEG_QUALITY, SCREENSHOT_QUALITY] if SCREENSHOT_FMT in ('jpg','jpeg') else []
        if cv2.imwrite(filename, image, params): return filename
    except: pass
    return None

# --- [UX ê°œì„ : í•˜ë‹¨ ì¸ë²¤í† ë¦¬ UI ê·¸ë¦¬ê¸°] ---
def draw_ui(image, h, w):
    # í•˜ë‹¨ ë°˜íˆ¬ëª… ë°” (ë†’ì´ ì¦ê°€: 80 -> 100)
    overlay = image.copy()
    bar_height = 100
    cv2.rectangle(overlay, (0, h - bar_height), (w, h), (0, 0, 0), -1)
    image = cv2.addWeighted(overlay, 0.6, image, 0.4, 0)
    
    # ì•„ì´í…œ ê°„ê²© ê³„ì‚°
    item_count = len(FILTER_ITEMS)
    spacing = w // (item_count + 1)
    
    for i, (item_key, label) in enumerate(zip(FILTER_ITEMS, FILTER_LABELS)):
        x_pos = spacing * (i + 1)
        # ì•„ì´í…œ ìœ„ì¹˜ ì¡°ê¸ˆ ìœ„ë¡œ ì¡°ì •
        y_pos = h - bar_height // 2 - 5
        
        # 1. ì»¤ì„œ í‘œì‹œ (ì„ íƒëœ ì•„ì´í…œ ê°•ì¡°)
        if i == current_cursor_index:
            # ì»¤ì„œ ë°•ìŠ¤
            box_w, box_h = 100, 60
            cv2.rectangle(image, (x_pos - box_w//2, y_pos - box_h//2), 
                         (x_pos + box_w//2, y_pos + box_h//2), (255, 255, 0), 2)
            
        # 2. í™œì„± ìƒíƒœ í‘œì‹œ (ì°©ìš© ì¤‘ì¸ ì•„ì´í…œ)
        is_active = item_key in active_filters
        text_color = (0, 255, 0) if is_active else (150, 150, 150) # ì°©ìš©:ì´ˆë¡, ë¯¸ì°©ìš©:íšŒìƒ‰
        
        # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° (ì¤‘ì•™ ì •ë ¬)
        image = put_korean_text(image, label, (x_pos, y_pos - 15), 
                               font_size=24, color=text_color, align='center')
        
        # ON/OFF ìƒíƒœ í…ìŠ¤íŠ¸
        status_text = "ON" if is_active else "OFF"
        status_color = (0, 255, 0) if is_active else (100, 100, 100)
        image = put_korean_text(image, status_text, (x_pos, y_pos + 20),
                               font_size=16, color=status_color, align='center')

    # ìƒë‹¨ ì •ë³´ í‘œì‹œ
    top_info = f"ë°°ê²½: {BACKGROUND_MODES[background_mode]}"
    image = put_korean_text(image, top_info, (20, 20), font_size=20, color=(255, 255, 255))
    
    # ì œìŠ¤ì²˜ ê°€ì´ë“œ ì¶”ê°€
    gesture_guide = "ì†ë™ì‘: âœŒ(ì´¬ì˜) ğŸ–(ì´ë™) âœŠ(ì„ íƒ) â˜(ë°°ê²½)"
    image = put_korean_text(image, gesture_guide, (w - 20, 70), font_size=18, color=(255, 200, 100), align='right')

    # ì¡°ì‘ ê°€ì´ë“œ (ìœ„ì¹˜ ìƒí–¥ ì¡°ì •)
    guide = "ì´ë™:[A/D]  ì„ íƒ:[SPACE]  ë°°ê²½:[TAB]  í¬ê¸°:[+/-]  íˆ¬ëª…ë„:[ [/] ]  ì´¬ì˜:[S]  ë…¹í™”:[R]"
    image = put_korean_text(image, guide, (w//2, h - 25), font_size=16, color=(200, 200, 200), align='center')
    
    return image

# --- [8. ë©”ì¸ ì‹¤í–‰] ---
cap = cv2.VideoCapture(0)
cv2.namedWindow('AR Filter Project', cv2.WINDOW_NORMAL)
cv2.resizeWindow('AR Filter Project', 1280, 720)

status_message = ""
message_timer = 0
recording = False
video_writer = None

while cap.isOpened():
    success, image = cap.read()
    if not success: break

    # ì„±ëŠ¥ ìµœì í™”: ì´ë¯¸ì§€ ì“°ê¸° ê¸ˆì§€
    image.flags.writeable = False
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # Face Mesh & Hands ì²˜ë¦¬
    results_face = face_mesh.process(image)
    results_hands = hands.process(image)
    
    image.flags.writeable = True
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    
    if background_mode != 0: image = apply_background(image, background_mode)
    
    h, w, c = image.shape
    
    # ì–¼êµ´ í•„í„° ì²˜ë¦¬
    if results_face.multi_face_landmarks:
        for face_landmarks in results_face.multi_face_landmarks:
            image = apply_filters(image, face_landmarks, active_filters, h, w)
            # ì… ë²Œë¦¼ íš¨ê³¼
            top_y = face_landmarks.landmark[13].y
            bot_y = face_landmarks.landmark[14].y
            if int(abs(top_y - bot_y) * h) > 40:
                cv2.putText(image, "Wow!", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), 5)

    # ì œìŠ¤ì²˜ ì²˜ë¦¬
    if results_hands.multi_hand_landmarks:
        for hand_landmarks in results_hands.multi_hand_landmarks:
            # ì† ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸° (ë””ë²„ê¹…ìš©)
            mp.solutions.drawing_utils.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)
            
            # ì¿¨íƒ€ì„ ì²´í¬
            if gesture_cooldown == 0:
                gesture = detect_gesture(hand_landmarks)
                
                if gesture != "NONE":
                    if gesture == "V": # ìŠ¤í¬ë¦°ìƒ·
                        f_name = "_".join(active_filters) if active_filters else "none"
                        if save_screenshot(image, f_name):
                            status_message = "âœŒ ì°°ì¹µ!"
                        gesture_cooldown = GESTURE_LOCK_TIME * 2 # ì´¬ì˜ì€ ì¿¨íƒ€ì„ ê¸¸ê²Œ
                        
                    elif gesture == "PALM": # ì´ë™ (ì˜¤ë¥¸ìª½ìœ¼ë¡œ)
                        current_cursor_index = (current_cursor_index + 1) % len(FILTER_ITEMS)
                        status_message = "ğŸ– ì´ë™"
                        gesture_cooldown = GESTURE_LOCK_TIME
                        
                    elif gesture == "FIST": # ì„ íƒ/í•´ì œ
                        selected_item = FILTER_ITEMS[current_cursor_index]
                        if selected_item in active_filters:
                            active_filters.remove(selected_item)
                            status_message = "âœŠ í•´ì œ"
                        else:
                            active_filters.append(selected_item)
                            status_message = "âœŠ ì¥ì°©"
                        gesture_cooldown = GESTURE_LOCK_TIME
                        
                    elif gesture == "POINT": # ë°°ê²½ ë³€ê²½
                        background_mode = (background_mode + 1) % len(BACKGROUND_MODES)
                        status_message = f"â˜ ë°°ê²½: {BACKGROUND_MODES[background_mode]}"
                        gesture_cooldown = GESTURE_LOCK_TIME
                    
                    message_timer = 30

    # ì¿¨íƒ€ì„ ê°ì†Œ
    if gesture_cooldown > 0:
        gesture_cooldown -= 1

    # UI ê·¸ë¦¬ê¸°
    image = draw_ui(image, h, w)
    
    # ìƒíƒœ ë©”ì‹œì§€
    if status_message and message_timer > 0:
        image = put_korean_text(image, status_message, (w//2, h//2), font_size=40, color=(0, 255, 255), align='center')
        message_timer -= 1
        
    if recording:
        # ë…¹í™” í‘œì‹œ ìœ„ì¹˜ ìš°ì¸¡ ìƒë‹¨ìœ¼ë¡œ ë³€ê²½
        cv2.circle(image, (w - 100, 40), 10, (0, 0, 255), -1)
        image = put_korean_text(image, "REC", (w - 70, 30), font_size=20, color=(0, 0, 255))
        if video_writer: video_writer.write(image)

    cv2.imshow('AR Filter Project', image)
    
    key = cv2.waitKey(5) & 0xFF
    if key == ord('q'): break
    
    # --- [UX ì¡°ì‘ í‚¤ ë§¤í•‘] ---
    elif key == ord('a') or key == ord('A'):
        current_cursor_index = (current_cursor_index - 1) % len(FILTER_ITEMS)
    elif key == ord('d') or key == ord('D'):
        current_cursor_index = (current_cursor_index + 1) % len(FILTER_ITEMS)
    elif key == ord(' '):
        selected_item = FILTER_ITEMS[current_cursor_index]
        if selected_item in active_filters:
            active_filters.remove(selected_item)
            status_message = "OFF"
        else:
            active_filters.append(selected_item)
            status_message = "ON"
        message_timer = 20
    
    elif key == 9: # Tab
        background_mode = (background_mode + 1) % len(BACKGROUND_MODES)
        status_message = f"ë°°ê²½: {BACKGROUND_MODES[background_mode]}"
        message_timer = 30
        
    # ê¸°ì¡´ ê¸°ëŠ¥ í‚¤ë“¤
    elif key == ord('s') or key == ord('S'):
        f_name = "_".join(active_filters) if active_filters else "none"
        if save_screenshot(image, f_name):
            status_message = "ì €ì¥ ì™„ë£Œ!"
            message_timer = 30
    elif key == ord('r') or key == ord('R'):
        if not recording:
            if not os.path.exists(RECORD_DIR): os.makedirs(RECORD_DIR)
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            fn = os.path.join(RECORD_DIR, f"rec_{ts}.mp4")
            video_writer = cv2.VideoWriter(fn, cv2.VideoWriter_fourcc(*RECORD_CODEC), 30, (w, h))
            recording = True
            status_message = "ë…¹í™” ì‹œì‘"
        else:
            recording = False
            if video_writer: video_writer.release()
            video_writer = None
            status_message = "ë…¹í™” ì €ì¥ë¨"
        message_timer = 30
    elif key == ord('0'):
        active_filters = []
        background_mode = 0
        status_message = "ì´ˆê¸°í™”"
        message_timer = 30
    elif key in (ord('+'), ord('=')): SIZE_SCALE = min(SIZE_MAX, SIZE_SCALE + SIZE_STEP)
    elif key in (ord('-'), ord('_')): SIZE_SCALE = max(SIZE_MIN, SIZE_SCALE - SIZE_STEP)
    elif key == ord(']'): ALPHA_SCALE = min(ALPHA_MAX, ALPHA_SCALE + ALPHA_STEP)
    elif key == ord('['): ALPHA_SCALE = max(ALPHA_MIN, ALPHA_SCALE - ALPHA_STEP)

cap.release()
if video_writer: video_writer.release()
cv2.destroyAllWindows()
